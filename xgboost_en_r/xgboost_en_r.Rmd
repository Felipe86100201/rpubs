---
title: "XGBoost en R"
author: "Juan Bosco Mendoza Vega"
date: "8/12/2019"
output: 
  html_document: 
    highlight: haddock
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment = "")
```

**XGBoost** o **Extreme Gradient Boosting**, es uno de los algoritmos de machine learning de tipo supervisado más usados en la actualidad. 

Este algoritmo se caracteriza por obtener buenos resultados de predicción con relativamente poco esfuerzo, incluso mejores que los devueltos por modelos más complejos computacionalmente, en particular para problemas con conjuntos de datos heterogéneos. 

XGBoost es entonces una herramienta muy útil para un data scientist, que cuenta con implementaciones para diferentes lenguajes y entornos de programación.

Este artículo revisaremos la implementación de XGBoost en R. No nos detendremos a revisar todos los ajustes posibles a los hiper parámetros de este algoritmo, pero vale la pena.

# Una introducción informal a XGBoost

XGBoost  Extreme Gradient Boosting es un algoritmo predictivo que utiliza el principio de *boosting*.

La idea general del *boosting* es generar múltiples modelos de predicción "débiles" de manera secuencial, cada uno tomando los resultados del modelo anterior, para generar un modelo "fuerte", con mejor poder predictivo y mayor estabilidad en sus resultados.

Esto ocurre usando el algoritmo de optimización *Gradient Descent* (descenso de gradiente). Los parámetros de cada modelo débil son ajustados iterativamente tratanto de encontrar el mínimo de una función objetivo. 

Es decir, que cada modelo tenga mejor capacidad predictiva que el anterior, hasta llegar a un punto en el que la diferencia entre modelos consecutivos es insignificante, lo cual nos indica que hemos encontrado el mejor modelo posible.

XGBoost usa como modelos débiles árboles de decisión,  que pueden ser usado para tareas de clasificación y de regresión.

Si quieres conocer más sobre este algoritmo, puedes leer definiciones más formales y dicusión sobre implementación en los siguientes artículos

* [Greedy Function Approximation: A Gradient Boosting Machine (Friedman, 2001)](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451)
* [Stochastic Gradient Boosting (Friedman, 1999)](https://astro.temple.edu/~msobel/courses_files/StochasticBoosting(gradient).pdf)


# Implementación en R
## Instalación
Necesitamos es instalar los paquetes que usaremos con `install.packages()`.

```{r instalar, eval=FALSE}
install.packages("xgboost")
install.packages("tidyverse")
install.packages("caret")
```

El paquete `xgboost` contiene la implementación de este tipo de modelo para R.

Por su parte, `tidyverse` es un meta paquete que carga varios paquetes a nuestro entorno de trabajo. Para nuestros fines, lo importante es que carga los siguientes:

* `dplyr`. Herramientas manipular, tranformar y organizar datos.
* `readr`. Facilita importar datos desde archivos.
* `purrr`. Agrega características de programación funcional.

Y de `caret` unicamente usaremos una función para generar 

Cargamos los paquetes a nuestro espacio de trabajo con `library()`.

```{r cargar paquetes}
library("xgboost")
library("tidyverse")
library("caret")
```

# Datos que usaremos 
El conjunto de datos que usaremos es conocido como **Agraricus** que contiene características de diferentes hongos y la que deseamos predecir es si son venenosos o no.

Como en la vida real es comun que tengas que lidiar con datos en formatos no convencionales y que requieren procesamiento antes de usarlos en un modelo predictivo. 

Por esta razón, en lugar de usar la versión de este conjunto de datos incluida en el paqute `xgboost` y que ya está lista para usar, utilizaremos una versión de estos mismos datos, disponible en el Machine Learning Repository de UCI.

* https://archive.ics.uci.edu/ml/datasets/Mushroom

He copiado los datos a un repositorio de github para asegurar que estés usando la misma versión que aparece en este artículo. Son dos archivos, uno con los datos y otro que contiene una descripción de ellos.

Descargamos ambos archivos a nuestra carpeta de trabajo usando la `download.files()`. Usamos el argumento `mode = "wb"` para asegurar que la descarga ocurra correctamente. 

```{r descarga_archivos, eval=FALSE}
download.file("https://raw.githubusercontent.com/jboscomendoza/rpubs/xgboost_r/xgboost_en_r/agaricus-lepiota.data",
              destfile = "agaricus-lepiota.data", mode = "wb")
download.file("https://raw.githubusercontent.com/jboscomendoza/rpubs/xgboost_r/xgboost_en_r/agaricus-lepiota.names",
              destifile = "agaricus-lepiota.names", mode = "wb")
```

# Exploración de los datos
Empezamos explorando el contenido de estos archivos. Podemos dar un vistazo a su contenido con algún procesador de texto externo, como notepad++ o gedit, pero también podemos hacerlo directamente en R.

Usamos las fuciones `read_lines()` y `head()` para ver los primeros renglones del archivo **"agaricus-lepiota.data"**.

```{r head data}
head(read_lines("agaricus-lepiota.data"))
```

Los datos se encuentran en una estructura tabular, separada por comas. Para fines prácticos, es equivalente a un archivo csv pero con una extensión diferente.

Pero, para evitar errores en la lectura, usaremos la función `read.table` para importar su contenido. Esta función no trata de convertir los datos a un tipo en particular, de modo que todo sea importado como caracter, que más arriba.

Llamamos esta función especificando que el delimitador de columnas es una coma. 

También convertiremos a un `tibble` para mejorar la compatibilidad con los paquetes del `tidyverse` y para mejorar la presentación en pantalla. Este paso es completamente opcional.

```{r lectura inicial}
original <- 
  read.table("agaricus-lepiota.data", sep = ",") %>% 
  tbl_df()
```

Veamos el resultado.

```{r ver original}
original
```

Nos falta el nombre de cada columna, es decir, de las variables o features. Aunque agregar nombres a las columnas no es estrictamente necesario. 

De hecho, es fecuente que tengas que trabajar con conjuntos de datos a los que se les ha ocultado intencionalmente el nombre de los features por seguridad o confidencialidad, entre otras razones.

Nosotros lo haremos porque, cuando se cuenta con ellas, nos pueden aportar insights sobre nuestros datos que pueden ser útiles para realizar buenas predicciones.

Además, es esencial conocer cuál de las columnas contiene nuestra variable objetivo, aquella que buscamos predecir.

Los nombres de las variables han sido obtenidos de la información contenida en el archivo `agaricus-lepiota.names` y son los siguientes. La variable objetivo es la primera de todas.

Hacemos la asignación de la siguiente manera.
```{r nombres}
nombres <- 
  c(
    "target", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", 
    "gill_attachment", "gill_spacing", "gill_size", "gill_color", "stalk_shape",
    "stalk_root", "stalk_surface_above_ring", "stalk_surface_below_ring", 
    "stalk_color_above_ring", "stalk_color_below_ring", "veil_type", 
    "veil_color", "ring_number", "ring_type", "spore_print_color", "population",
    "habitat"
  )

names(original) <- nombres
```

Nuestro resultado.

```{r ver nombres original}
head(original)
```

# Procesamiento
Hasta aquí, todo luce bien, sin embargo, `xgboost` requiere matrices numéricas para funcionar correctamente, así que tenemos que convertir nuestra columnas, de datos de tipo caracter a tipo numérico.

Usamos la función `map_df()` de `purrr` para realizar esta conversión de manera eficiente.

Cada columna será convertida a factor con `as.factor()`, después a número con `as.numeric()` y finalmente restamos 1 al resultado de la conversión porque xgboost, para clasificación binaria, espera valores de 0 y 1 para la variable objetivo. 

Si no realizamos esta operación, nuestro valores numéricos iniciarán en 1 y esto hará que xgboost devuelva resultados inválidos. Además, si intentamos convertir directamente de caracter a numérico, obtendremos columnas llenas de `NA`, por eso es necesario un paso intermedio, que aprovecha las coerciones de tipo de datos en R.

```{r convertir}
original <- map_df(original, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})
```

Nuestro_resultado es el siguiente.

```{r}
head(original)
```

Creamos una lista llamada **hongo**. Esto no es estrictamente necesario, pero a mi me resulta más fácil guardar todos los objetos relacionados con un mismo proceso en una lista. De este modo, mi espacio de trabajo es más fácil de manejar y, mientras sea consistente con las estructura de esta lista, puedo reproducir mis análisis y hacer ajustes de manera clara y ordenada.

Los primero que haremos es guardar nuestros datos originales en esta lista.

```{r}
hongo <- list()

hongo$original <- original
```

Como es el caso con todos los algoritmos de predicción supervisados, necesitamos dividir nuestros datos en un conjunto de entrenamiento, que aprenderá las características de los datos y generara un modelo de predicción; y un conjunto de prueba, que usaremos para validar el modelo generado.

# Sets de entrenamiento y prueba
Para generar nuestro set de entrenamiento usaremos la función `sample_frac()` de `dplyr` para extraer una muestra aleatoria del 70% de nuestros datos originales.

Utilizamos `set.seed()` para asegurar que estos resultados son replicables.

```{r}
set.seed(1919)
hongo$train_df <- sample_frac(original, size = 0.7)
```

El resto de los datos, 30% de ellos, será el conjunto de prueba (test). Usamos `setdiff()` para seleccionarlos.

```{r}
hongo$test_df <- setdiff(hongo$original, hongo$train_df)
```

En ambos casos, hemos guardado los resultados en nuestra lista `hongo`. Veamos el tamaño de estos conjuntos de datos con la función `dim`.

```{r size_data}
dim(hongo$train)
dim(hongo$test)
```

Como ya lo mencionamos, la implementación XGBoost de R requiere que los datos que usemos sean matrices, específicamente de tipo DMatrix, así que necesitamos convertir nuestros sets de entrenamiento y prueba a este tipo de estructura.

# Convertir a DMatrix

Para convertir nuestros datos usaremos la función `xgb.DMatrix()` de `xgboost`. Esta función espera una matriz numérica como primer argumento y también se pueden especificar algunos atributos adicionales al objeto que devolverá.

Nosotros definiremos el atributo `label` para identificar la variable objetivo en nuestros datos.

Al usar esta función es muy importante que tu `data` no incluya la columna con la variable objetivo, de lo contrario, obtendrás una precisión perfecta en tus predicciones, la cual no te servirá de nada con datos nuevos.

Entonces, lo que haremos será quitar la columna con la variable objetivo usando la función `select()` de `dplyr`, convertiremos nuestros datos a matriz con `as.matrix()`, convertimos esta matriz con `xgb.Dmatrix()` y guardamos el resultado en nuestra lista `hongo`.

```{r}
hongo$train_mat <- 
  hongo$train_df %>% 
  select(-target) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = hongo$train_df$target)
```

Nuestro resultado es el siguiente.

```{r}
hongo$train_mat
```

Hacemos lo mismo con nuestro set de entrenamiento.

```{r}
hongo$test_mat <- 
  hongo$test_df %>% 
  select(-target) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = hongo$test_df$target)
```

Y obtenemos el siguiente resultado.

```{r}
hongo$test_mat
```


¡Listo! Ya hemos hecho la parte más laboriosa de todo el proceso. Prodemos comenzar con el entrenamiento del modelo predictivo.

## Entrenamiento del modelo predictivo

Para entrenar un modelo usamos la función `xgboost()`.

Podemos especificar una amplia cantidad de hiper parámetros para nuestro modelo, para este ejemplo introductorio haremos ajustes a los siguientes:

* **objective**: El tipo de tarea de clasificación que realizaremos. Para clasificación binaria, que es nuestro ejemplo, usamos "binary:logistic".
* **nround**: El número de iteraciones que se realizarán antes de detener el proceso de ajuste. Un mayor número de iteraciones generalmente devuelve mejores resultados de predicción.
* **max.depth**: "Profundidad", número de nodos de bifurcación, de los árboles de de decisión usados en el ajuste. Aunque una mayor profundidad puede devolver mejores resultados, también puede resultar en *overfitting* (sobre ajuste).
* **eta**: La tasa de aprendizaje del modelo. Un mayor valor llega a un "mejor modelo" más rápidamente, pero puede pasarse del valor óptimo de la función objetivo. En cambio, una valor pequeño puede nunca llegar al valor óptimo de la función objetivo. En ambos casos, esto afecta el desempeño de nuestros modelos con datos nunca antes vistos.
* **nthread**: El número de hilos computacionales que serán usados en el proceso de entrenamiento. Generalmente, se refiere a los núcleos del procesador de tu equipo de cómputo, pero también pueden ser los núcleos de un GPU.

Como los datos que usaremos son sencillos, usaremos valores muy conservadores para todos estos hiper parámetros.

Entrenamos el modelo y lo guardamos nuestro modelo en `hongo$modelo_01`.

Para datos más complejos, este proceso puede ser tardado. Además, puedes el progreso del entrenamiento conforme este ocurre.

```{r modelo_01}
hongo$modelo_01 <- xgboost(data = hongo$train_mat, 
                           objective = "binary:logistic",
                           nrounds = 10, max.depth = 2, eta = 0.3, nthread = 2)
```

Nuestro resultado es el siguiente. Notarás que la salida incluye información de los hiperparámetros y los datos que usamos así como un resumen de la ejecución del modelo.

```{r}
hongo$modelo_01
```

## Generación de predicciones
Usamos la función `predict()` con el set de prueba para generar las predicciones de nuestro modelo.

Guardamos las predicciones en la lista que hemos creado.

```{r}
hongo$predict_01 <- predict(hongo$modelo_01, hongo$test_mat)
```

Nuestro resultado es un vector de valores numéricos. Cada uno representa la probabilidad de que un caso en particular pertenezca al valor 1 de nuestra variable objetivo. 

```{r}
head(hongo$predict_01)
```

Para este ejemplo, vamos a tomar las probabilidades a 0.5 como una predicción de pertenencia al valor 1 de nuestra variable objetivo.

```{r}
head(hongo$predict_01 > 0.5)
```

## Evaluación del modelo
Entonces todo lo que necesitamos hacer es comparar estas predicciones con las categorías reales de nuestro set de prueba.

Usaremos la funcuión `confusionMatrix()` de `caret` para generar un matriz de  confusión y evaluar el desempeño de nuestro modelo.

La manera más sencilla de utilizar esta función es darle como argumento una objeto de tipo `table`. 

Unimos nuestras predicciones en `hongo$predict_01` y los valores reales en `hongo$test_df$target` para generar un data frame y con ella un objeto `table`.

```{r}
cbind(hongo$predict_01 > 0.5, hongo$test_df$target) %>% 
  data.frame() %>% 
  table() %>% 
  confusionMatrix()
```

Nada mal. Tuvimos una precisión del 98% y sobresale que tuvimos un valor predictivo par la clase negativa, un hongo no venenoso, del 100%.

Como podrás imaginar, con datos reales rara vez obtenemos resultados tan buenos con tan poco esfuerzo, pero si comparas estos resultados contra los de árboles de decisión convencionales, notarás una gran diferencia en desempeño a favor de XGboost.

Después de preparar nuestros datos, la tarea que más tiempo consume al usr este modelo es encontrar los mejores hiper parámetros para alcanzar la mayor precisión posible de un modelo.

Veamos que pasa si ajustamos nuestros hiper parámetros con un segundo modelo.

## Segundo modelo.

Este segundo modelo tiene un número de iteraciones mayor que el anterior, de 100 en lugar de 10, y una mayor profundidad en los árboles generados, de 2 a 4.

Además, hemos ajustado el hiper parámetro `early_stopping_rounds = 10`, para que el entrenamiento se detenga si después de diez iteraciones consecutivas no se mejora el modelo. Este hiper parámetro es sumamente útil para acortar el tiempo de entrenamiento de un modelo, pues evita que el proceso continue si ya no se están obteniendo mejores resultados de predicción.

En este mismo bloque de código generamos una matriz de confusión para evaluar nuestro segundo modelo.

```{r modelo_02}
hongo$modelo_02 <- xgboost(data = hongo$train_mat, 
                           objective = "binary:logistic",
                           nrounds = 100, max.depth = 4, eta = 0.3, nthread = 2, 
                           early_stopping_rounds = 10)


hongo$predict_02 <- predict(hongo$modelo_02, hongo$test_mat)

cbind(hongo$predict_02 > 0.5, hongo$test_df$target) %>% 
  data.frame() %>% 
  table() %>% 
  confusionMatrix()
```

El entrenamiento se ha detenido después de 17 iteraciones y ha producido un modelo con una precisión del 100%. Nada mal.

# Para concluir

En este artículo hemos revisado, de manera general, la implementación para **R** del algoritmo **XGBoost**. En el proceso, también dimos un vistazo rápido al proceso para preparar datos con formatos no convencionales para ser usados en este algoritmo.

Esta revisión no ha sido exahustiva, por lo que hay algunos temas que es importante que estudies para obtener mejores resultados al usar XGBoost:

* En nuestro ejemplo, tomamos los datos tal cual los obtuvimos. En la práctica, es esencial realizar una exploración mucho más profunda de los datos, por ejemplo, analizar las correlaciones entre variables, el comportamiento de datos perdidos e identificar imbalances en la variable objetivo.
* Los hiper parámetros que usamos en nuestro ejemplo no son los únicos que tiene XGBoost. Conocerlos todos y entender cómo cambian el comportamiento del algoritmo te da herramientas para hacer ajustes razonados y obtener mejores resultados de predicción.
* Sólo realizamos una tarea de clasificación binaria, pero no es la única que puede revisar XGBoost. Cada una de ellas requiere de una preparación de datos y ajuste de hiper parámetros diferente.

Si quieres conocer más sobre estos temas un buen punto de partida es la documentación de XGBoost.

* [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/index.html)

--- 

Consultas, dudas, comentarios y correcciones son bienvenidas:

  *  jboscomendoza@gmail.com

El código y los datos usados en este documento se encuentran en Github:

  *  https://github.com/jboscomendoza/rpubs/tree/master/xgboost_en_r


